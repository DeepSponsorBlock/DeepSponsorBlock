{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function, division\n",
    "import sys\n",
    "sys.path.append(\"../\")\n",
    "\n",
    "from dsbtorch import scan_dataset, IterableVideoSlidingWindowDataset, VideoSlidingWindowDataset\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.optim import lr_scheduler\n",
    "import numpy as np\n",
    "import torchvision\n",
    "from torchvision import datasets, models, transforms\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "import os\n",
    "import copy\n",
    "import pathlib\n",
    "import pickle\n",
    "\n",
    "import tqdm\n",
    "\n",
    "from CNN_RNN import *\n",
    "from torch.utils.tensorboard import SummaryWriter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "scans/home/ubuntu/data/toy_dataset/train/ws1-psr1-nsr1.pkl\n",
      "scans/home/ubuntu/data/toy_dataset/dev/ws1-psr1-nsr1.pkl\n"
     ]
    }
   ],
   "source": [
    "plt.ion()   # interactive mode\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "# device = torch.device(\"cpu\")\n",
    "\n",
    "t = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "# data_dir = \"/home/ubuntu/data/dataset/\"\n",
    "data_dir = \"/home/ubuntu/data/toy_dataset/\"\n",
    "# data_dir = \"/home/ubuntu/data/dataset_imagefolder_sampled_1000/\"\n",
    "# data_dir = \"/home/ubuntu/data/toy_dataset_imagefolder/\"\n",
    "is_imagefolder = False\n",
    "sampling_rate = 1\n",
    "\n",
    "dataset_names = ['train', 'dev'] #, 'test']\n",
    "\n",
    "window_size = 1\n",
    "positive_sampling_rate = sampling_rate\n",
    "negative_sampling_rate = sampling_rate\n",
    "\n",
    "if not is_imagefolder:\n",
    "    scanned_datasets = {}\n",
    "    for x in dataset_names:\n",
    "        sddir = pathlib.Path(\"scans/\" + os.path.join(data_dir, x))\n",
    "        sdfile = sddir / (\"ws%d-psr%d-nsr%d.pkl\" % (window_size, positive_sampling_rate, negative_sampling_rate))\n",
    "        print(sdfile)\n",
    "        if not sdfile.exists():\n",
    "            raise ValueError(\"You need to use the ScanDatasets notebook first to scan & pickle the dataset.\")\n",
    "        with open(sdfile, 'rb') as f:\n",
    "            scanned_datasets[x] = pickle.load(f)\n",
    "            \n",
    "if is_imagefolder:\n",
    "    datasets = {x: torchvision.datasets.ImageFolder(os.path.join(data_dir, x), transform=t) for x in dataset_names}\n",
    "else:\n",
    "    datasets = {x: VideoSlidingWindowDataset(scanned_datasets[x], t) for x in dataset_names}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 1024\n",
    "dataloaders = {x: torch.utils.data.DataLoader(datasets[x], batch_size=batch_size, num_workers=6, pin_memory=True) for x in dataset_names}\n",
    "dataset_sizes = {x: len(datasets[x]) for x in dataset_names}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_to_onehot(start_preds, end_preds):\n",
    "    \"\"\"\n",
    "    Converts preds into a one-hot vector with 1's for sponsored frames\n",
    "    \"\"\"\n",
    "    onehot_preds = np.zeros(len(start_preds))\n",
    "\n",
    "    # Transform into tuples of (timestamp, is_start)\n",
    "    start_preds = [(idx, True) for idx in torch.nonzero(start_preds, as_tuple=False)]\n",
    "    end_preds = [(idx, False) for idx in torch.nonzero(end_preds, as_tuple=False)]\n",
    "    timestamps = sorted(start_preds + end_preds, key=lambda t: t[0])\n",
    "\n",
    "    seg_start = 0\n",
    "    in_seg = False\n",
    "    for t, is_start in timestamps:\n",
    "        if is_start and not in_seg:\n",
    "            seg_start = t\n",
    "            in_seg = True\n",
    "        elif not is_start and in_seg:\n",
    "            onehot_preds[seg_start : t+1] = 1\n",
    "            in_seg = False\n",
    "\n",
    "    return torch.tensor(onehot_preds, dtype=torch.long)\n",
    "\n",
    "\n",
    "def IOU(preds, labels):\n",
    "    intersection = torch.nonzero(preds * labels, as_tuple=False).shape[0]\n",
    "    union = torch.nonzero(preds + labels, as_tuple=False).shape[0]\n",
    "    return intersection / union if union != 0 else 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, criterion, optimizer, scheduler, output_path, num_epochs=25, beta2=0.25, print_every_n=0):\n",
    "    writer = SummaryWriter()\n",
    "\n",
    "    since = time.time()\n",
    "    cnn_encoder, rnn_decoder = model\n",
    "\n",
    "    best_decoder_wts = copy.deepcopy(rnn_decoder.state_dict())\n",
    "    best_iou = 0.0\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        print('\\n\\nEpoch {}/{}'.format(epoch, num_epochs - 1))\n",
    "        print('-' * 10)\n",
    "        \n",
    "        epoch_loss = {}\n",
    "        epoch_iou = {}\n",
    "\n",
    "        # Each epoch has a training and validation phase\n",
    "        for phase in ['train', 'dev']:\n",
    "            cnn_encoder.eval()\n",
    "            if phase == 'train':\n",
    "                rnn_decoder.train()\n",
    "                print('Training for one epoch.')\n",
    "                print('-' * 8)\n",
    "            else:\n",
    "                rnn_decoder.eval()\n",
    "                print('Evaluating model.')\n",
    "                print('-' * 8)\n",
    "\n",
    "            running_loss = 0.0\n",
    "            total_iou = 0\n",
    "\n",
    "            i = 0\n",
    "            video_start = time.time()\n",
    "            \n",
    "            sd = scanned_datasets[phase]\n",
    "\n",
    "            lengths = list(np.diff(np.array(sd.cumulative_indices + [sd.n_indices])))\n",
    "\n",
    "            # Reverse them to use as a stack.\n",
    "            lengths.reverse()\n",
    "\n",
    "            encoder_outputs = []\n",
    "            acc_labels = []\n",
    "            for imgs, lbls in tqdm.tqdm(dataloaders[phase]):\n",
    "                imgs = torch.reshape(imgs, (-1, 3, 144, 256)).to(device)\n",
    "                encoder_outputs.append(encoder(imgs))\n",
    "                acc_labels.append(torch.reshape(lbls, (-1, )).to(device))\n",
    "\n",
    "                while lengths and sum(x.shape[0] for x in encoder_outputs) >= lengths[-1]:\n",
    "                    combined_encoder_outputs = torch.cat(encoder_outputs).to(device)\n",
    "                    combined_labels = torch.cat(acc_labels).to(device)\n",
    "                    \n",
    "                    length = lengths.pop()\n",
    "                    \n",
    "                    encoder_outputs = [combined_encoder_outputs[length:]]\n",
    "                    acc_labels = [combined_labels[length:]]\n",
    "                    \n",
    "                    cnn_outputs, labels = (combined_encoder_outputs[:length], combined_labels[:length])\n",
    "                    \n",
    "                    # labels = torch.reshape(labels, (-1, ))\n",
    "                    diffs = (labels[1:] - labels[:-1]).to(device)\n",
    "\n",
    "                    # 1's for frames where the start of a sponsored segment occurs\n",
    "                    start_labels = torch.cat((torch.tensor([labels[0]]).to(device), diffs)) == 1\n",
    "                    start_labels = start_labels.float()\n",
    "                    start_labels = start_labels.to(device)\n",
    "\n",
    "                    # 1's for frames where the end of a sponsored segment occurs\n",
    "                    end_labels = torch.cat((diffs, torch.tensor([labels[-1]]).to(device))) == -1\n",
    "                    end_labels = end_labels.float()                \n",
    "                    end_labels = end_labels.to(device)\n",
    "\n",
    "                    # zero the parameter gradients\n",
    "                    optimizer.zero_grad()\n",
    "\n",
    "                    # forward\n",
    "                    # track history if only in train\n",
    "                    with torch.set_grad_enabled(phase == 'train'):\n",
    "                        start_probs, end_probs = rnn_decoder(cnn_outputs)\n",
    "                        start_probs = torch.reshape(start_probs, (-1,)).to(device)\n",
    "                        end_probs = torch.reshape(end_probs, (-1,)).to(device)\n",
    "\n",
    "                        start_preds = torch.gt(start_probs, 0.5).to(device)\n",
    "                        end_preds = torch.gt(end_probs, 0.5).to(device)\n",
    "\n",
    "                        loss = criterion(start_probs, start_labels) + criterion(end_probs, end_labels)\n",
    "\n",
    "                        # backward + optimize only if in training phase\n",
    "                        if phase == 'train':\n",
    "                            loss.backward()\n",
    "                            optimizer.step()\n",
    "\n",
    "                    # statistics\n",
    "                    running_loss += loss.item()\n",
    "                    preds = convert_to_onehot(start_preds, end_preds).to(device)\n",
    "                    total_iou += IOU(preds, labels)\n",
    "\n",
    "                    if print_every_n > 0 and i % print_every_n == 0 and i > 0:\n",
    "                        print(\"Video number \", i + 1)\n",
    "                        print(\"Time since last update: \", time.time() - video_start)\n",
    "                        video_start = time.time()\n",
    "                    i += 1\n",
    "                    \n",
    "            assert not lengths\n",
    "            assert sum(x.shape[0] for x in encoder_outputs) == 0\n",
    "            assert sum(x.shape[0] for x in acc_labels) == 0\n",
    "                \n",
    "            if phase == 'train':\n",
    "                scheduler.step()\n",
    "\n",
    "            epoch_loss[phase] = running_loss / i\n",
    "            epoch_iou[phase] = total_iou / i\n",
    "            \n",
    "            writer.add_scalar(\"Loss/\" + phase, epoch_loss[phase], epoch)\n",
    "            writer.add_scalar(\"IOU/\" + phase, epoch_iou[phase], epoch)\n",
    "\n",
    "            print('{} Loss: {:.4f} IOU: {:.4f}'.format(\n",
    "                phase, epoch_loss[phase], epoch_iou[phase]))\n",
    "\n",
    "            # deep copy the model\n",
    "            if phase == 'dev' and epoch_iou[phase] > best_iou:\n",
    "                best_iou = epoch_iou[phase]\n",
    "                best_decoder_wts = copy.deepcopy(rnn_decoder.state_dict())\n",
    "                \n",
    "    time_elapsed = time.time() - since\n",
    "    print('Training complete in {:.0f}m {:.0f}s'.format(\n",
    "        time_elapsed // 60, time_elapsed % 60))\n",
    "    print('Best val F score: {:4f}'.format(best_iou))\n",
    "\n",
    "    # Save and load best model weights\n",
    "    rnn_decoder.load_state_dict(best_decoder_wts)\n",
    "    torch.save(rnn_decoder.state_dict(), output_path + \".decoder\")\n",
    "    return (cnn_encoder, rnn_decoder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder = ResCNN(\"/home/ubuntu/data/DeepSponsorBlock/results/attempt2-resnet50-sr10-sgd-lr10-decay1.weights\")\n",
    "decoder = nn.Sequential(\n",
    "    Embedder(encoder.num_ftrs),\n",
    "    DecoderRNN()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|          | 0/56 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Epoch 0/6\n",
      "----------\n",
      "Evaluating model.\n",
      "--------\n",
      "Lengths 50\n",
      "Total Length 56903\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 56/56 [04:41<00:00,  5.03s/it]\n",
      "  0%|          | 0/56 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dev Loss: 1.3678 IOU: 0.0000\n",
      "\n",
      "\n",
      "Epoch 1/6\n",
      "----------\n",
      "Evaluating model.\n",
      "--------\n",
      "Lengths 50\n",
      "Total Length 56903\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  4%|▎         | 2/56 [00:12<06:12,  6.91s/it]"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-8-8e43f54a379e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0mexp_lr_scheduler\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlr_scheduler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mStepLR\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstep_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m7\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgamma\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mencoder\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdecoder\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexp_lr_scheduler\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'../results/rnn.weights'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_epochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m7\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprint_every_n\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-6-a450356716e2>\u001b[0m in \u001b[0;36mtrain_model\u001b[0;34m(model, criterion, optimizer, scheduler, output_path, num_epochs, beta2, print_every_n)\u001b[0m\n\u001b[1;32m     47\u001b[0m                 \u001b[0mimgs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimgs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m144\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m256\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     48\u001b[0m                 \u001b[0mencoder_outputs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mencoder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimgs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 49\u001b[0;31m                 \u001b[0macc_labels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlbls\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     50\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m                 \u001b[0;32mwhile\u001b[0m \u001b[0mlengths\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mencoder_outputs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>=\u001b[0m \u001b[0mlengths\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "encoder = encoder.to(device)\n",
    "decoder = decoder.to(device)\n",
    "\n",
    "criterion = nn.BCELoss()\n",
    "\n",
    "# Observe that all parameters are being optimized\n",
    "optimizer = optim.SGD(decoder.parameters(), lr=0.001, momentum=0.9)\n",
    "\n",
    "# Decay LR by a factor of 0.1 every 7 epochs\n",
    "exp_lr_scheduler = lr_scheduler.StepLR(optimizer, step_size=7, gamma=0.1)\n",
    "\n",
    "model = train_model((encoder, decoder), criterion, optimizer, exp_lr_scheduler, '../results/rnn.weights', num_epochs=7, print_every_n=0)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Environment (conda_pytorch_latest_p36)",
   "language": "python",
   "name": "conda_pytorch_latest_p36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
