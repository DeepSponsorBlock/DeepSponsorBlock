{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function, division\n",
    "import sys\n",
    "sys.path.append(\"../\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "import numpy as np\n",
    "import os\n",
    "import pathlib\n",
    "import pickle\n",
    "import time\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.optim import lr_scheduler\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "import torchvision\n",
    "\n",
    "import dsbfetch\n",
    "import dsbtorch\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "torchvision.set_image_backend('accimage')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = \"/home/ubuntu/data/dataset\"\n",
    "dataset_names = ['train', 'dev']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "window_size = 1\n",
    "sampling_rate = 1\n",
    "positive_sampling_rate = sampling_rate\n",
    "negative_sampling_rate = sampling_rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scanned_datasets = {}\n",
    "for x in dataset_names:\n",
    "    sddir = pathlib.Path(\"scans/\" + os.path.join(data_dir, x))\n",
    "    sdfile = sddir / (\"ws%d-psr%d-nsr%d.pkl\" % (window_size, positive_sampling_rate, negative_sampling_rate))\n",
    "    print(sdfile)\n",
    "    if not sdfile.exists():\n",
    "        raise ValueError(\"You need to use the ScanDatasets notebook first to scan & pickle the dataset.\")\n",
    "    with open(sdfile, 'rb') as f:\n",
    "        scanned_datasets[x] = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "datasets = {x: dsbtorch.VideoSlidingWindowDataset(scanned_datasets[x], dsbtorch.DEFAULT_TRANSFORM) for x in dataset_names}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(\"Training example count: %d\" % len(datasets['train']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 128\n",
    "dataloaders = {x: torch.utils.data.DataLoader(datasets[x], batch_size=batch_size, num_workers=6, pin_memory=True) for x in dataset_names}\n",
    "dataset_sizes = {x: len(datasets[x]) for x in dataset_names}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def running_totals(preds, labels, tp, tn, fp, fn):\n",
    "    tp += torch.sum((preds == labels) * (labels == 1)).item()\n",
    "    tn += torch.sum((preds == labels) * (labels == 0)).item()\n",
    "    fp += torch.sum((preds != labels) * (labels == 0)).item()\n",
    "    fn += torch.sum((preds != labels) * (labels == 1)).item()\n",
    "    return (tp, tn, fp, fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, criterion, optimizer, scheduler, output_path, num_epochs=25, beta2=0.25, print_every_n=0, writer=None, hyperparams=None):\n",
    "    writer = SummaryWriter(comment=\"sr%d-%s-lr%d-decay%d.weights\" % (sampling_rate, hyperparams['optimizer'], int(hyperparams['learning_rate'] * 1000), int(hyperparams['lr_decay'])))\n",
    "    \n",
    "    since = time.time()\n",
    "\n",
    "    best_model_wts = copy.deepcopy(model.state_dict())\n",
    "    best_fscore = 0.0\n",
    "    \n",
    "    epoch_loss = {}\n",
    "    epoch_fscore = {}\n",
    "    epoch_accuracy = {}\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        print('\\n\\nEpoch {}/{}'.format(epoch, num_epochs - 1))\n",
    "        print('-' * 10)\n",
    "\n",
    "        # Each epoch has a training and validation phase\n",
    "        for phase in ['train', 'dev']:\n",
    "            if phase == 'train':\n",
    "                model.train()  # Set model to training mode\n",
    "                print('\\nTraining phase.')\n",
    "                print('-' * 8)\n",
    "            else:\n",
    "                model.eval()   # Set model to evaluate mode\n",
    "                print('\\nEvaluation phase.')\n",
    "                print('-' * 8)\n",
    "\n",
    "            running_loss = 0.0\n",
    "            tp, tn, fp, fn = 0, 0, 0, 0\n",
    "\n",
    "            i = 0\n",
    "            batch_start = time.time()\n",
    "            # Iterate over data.\n",
    "            for inputs, labels in dataloaders[phase]:                        \n",
    "                inputs = torch.reshape(inputs, (-1, 3, 144, 256))\n",
    "                inputs = inputs.to(device)\n",
    "                labels = torch.reshape(labels, (-1, ))\n",
    "                labels = labels.long()\n",
    "                labels = labels.to(device)\n",
    "\n",
    "                # zero the parameter gradients\n",
    "                optimizer.zero_grad()\n",
    "\n",
    "                # forward\n",
    "                # track history if only in train\n",
    "                with torch.set_grad_enabled(phase == 'train'):\n",
    "                    outputs = model(inputs)\n",
    "                    _, preds = torch.max(outputs, 1)\n",
    "                    loss = criterion(outputs, labels)\n",
    "\n",
    "                    # backward + optimize only if in training phase\n",
    "                    if phase == 'train':\n",
    "                        loss.backward()\n",
    "                        optimizer.step()\n",
    "\n",
    "                # statistics\n",
    "                running_loss += loss.item() * inputs.size(0)\n",
    "                tp, tn, fp, fn = running_totals(preds, labels.data, tp, tn, fp, fn)\n",
    "                \n",
    "                if (print_every_n > 0 and i % print_every_n == 0 and i > 0) or i == len(dataloaders[phase]) - 1:\n",
    "                    td = time.time() - batch_start\n",
    "                    print(\"Batch number \", i)\n",
    "                    print(\"Total images used this epoch: \", batch_size * print_every_n)\n",
    "                    print(\"Statistics: \", tp, tn, fp, fn)\n",
    "                    print(\"Time since last update: \", td)\n",
    "                    print(\"Time per 1000 images:\", td * 1000 / (batch_size * print_every_n))\n",
    "                    batch_start = time.time()\n",
    "\n",
    "                i += 1\n",
    "                \n",
    "            if phase == 'train':\n",
    "                scheduler.step()\n",
    "\n",
    "            epoch_loss[phase] = running_loss / dataset_sizes[phase]\n",
    "            epoch_fscore[phase] = (1 + beta2) * tp / ((1 + beta2) * tp + beta2 * fn + fp)\n",
    "            epoch_accuracy[phase] = (tp + tn) / (tp + tn + fp + fn)\n",
    "            \n",
    "            writer.add_scalar(\"Loss/\" + phase, epoch_loss[phase], epoch)\n",
    "            writer.add_scalar(\"FScore/\" + phase, epoch_fscore[phase], epoch)\n",
    "            writer.add_scalar(\"Accuracy/\" + phase, epoch_accuracy[phase], epoch)\n",
    "\n",
    "            print('{} Loss: {:.4f} F0.5: {:.4f}  Acc: {:.4f}'.format(\n",
    "                phase, epoch_loss[phase], epoch_fscore[phase], epoch_accuracy[phase]))\n",
    "\n",
    "            # deep copy the model\n",
    "            if phase == 'dev' and epoch_fscore[phase] > best_fscore:\n",
    "                best_fscore = epoch_fscore[phase]\n",
    "                best_model_wts = copy.deepcopy(model.state_dict())\n",
    "                torch.save(best_model_wts, output_path + str(epoch))\n",
    "\n",
    "    time_elapsed = time.time() - since\n",
    "    print('Training complete in {:.0f}m {:.0f}s'.format(\n",
    "        time_elapsed // 60, time_elapsed % 60))\n",
    "    print('Best val F score: {:4f}'.format(best_fscore))\n",
    "    \n",
    "    if hyperparams:\n",
    "        writer.add_hparams(\n",
    "            hyperparams,\n",
    "            {\n",
    "                \"H-Loss/Train\": epoch_loss['train'],\n",
    "                \"H-Loss/Dev\": epoch_loss['dev'],\n",
    "                \"H-FScore/Train\": epoch_fscore['train'],\n",
    "                \"H-FScore/Dev\": epoch_fscore['dev'],\n",
    "                \"H-Accuracy/Train\": epoch_accuracy['train'],\n",
    "                \"H-Accuracy/Dev\": epoch_accuracy['dev'],\n",
    "            })\n",
    "        \n",
    "    writer.close()\n",
    "\n",
    "    # Save and load best model weights\n",
    "    model.load_state_dict(best_model_wts)\n",
    "    torch.save(model.state_dict(), output_path)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs = 21\n",
    "\n",
    "for optname in ['sgd']: # ['adam', 'sgd']:\n",
    "    for lr in [0.01]: # [1.0, 0.1, 0.01, 0.001]:\n",
    "        for decay in [True]: # [True, False]:\n",
    "            model = dsbtorch.ResCNNClassifier().to(device)\n",
    "\n",
    "            criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "            # Observe that all parameters are being optimized\n",
    "            if optname == 'adam':\n",
    "                optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "            else:\n",
    "                optimizer = optim.SGD(model.parameters(), lr=lr, momentum=0.9)\n",
    "\n",
    "            # Decay LR by a factor of 0.1 every 7 epochs\n",
    "            exp_lr_scheduler = lr_scheduler.StepLR(optimizer, step_size=7, gamma=(0.1 if decay else 1))\n",
    "            \n",
    "            hyperparams = {'optimizer': optname, 'learning_rate': lr, 'lr_decay': decay}\n",
    "\n",
    "            outpath = \"../results/search-sr%d-%s-lr%d-decay%d.weights\" % (sampling_rate, optimizer, int(lr * 1000), int(decay))\n",
    "            model = train_model(model, criterion, optimizer, exp_lr_scheduler, outpath, num_epochs=num_epochs, print_every_n=50, hyperparams=hyperparams)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
